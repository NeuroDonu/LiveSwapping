# ‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —É—Å–∫–æ—Ä–µ–Ω–∏—é LiveSwapping –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

## üìö –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–°–∏—Å—Ç–µ–º–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞](#-—Å–∏—Å—Ç–µ–º–Ω–∞—è-–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞)
2. [TensorRT –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è](#-tensorrt-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
3. [GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ](#-gpu-—É—Å–∫–æ—Ä–µ–Ω–∏–µ)
4. [–í—ã–±–æ—Ä –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤](#-–≤—ã–±–æ—Ä-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤)
5. [–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π](#-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è-–º–æ–¥–µ–ª–µ–π)
6. [–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã](#-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏-—Å–∏—Å—Ç–µ–º—ã)
7. [–ë–µ–Ω—á–º–∞—Ä–∫–∏ –∏ —Ç–µ—Å—Ç—ã](#-–±–µ–Ω—á–º–∞—Ä–∫–∏-–∏-—Ç–µ—Å—Ç—ã)

---

## ü©∫ –°–∏—Å—Ç–µ–º–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞

### –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
from liveswapping.utils.gpu_utils import print_gpu_info, get_optimal_config

# –ü–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
print_gpu_info()

# –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
config = get_optimal_config()
print(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π batch size: {config['recommended_batch_size']}")
print(f"–°–º–µ—à–∞–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {config['use_mixed_precision']}")
print(f"–ü–∞–º—è—Ç—å GPU: {config['memory_gb']:.1f} GB")
```

### –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞

```python
# performance_check.py
import torch
import time
import numpy as np
from liveswapping.ai_models.models import get_optimal_provider, load_model

def comprehensive_benchmark():
    """–ü–æ–ª–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ —Å–∏—Å—Ç–µ–º—ã."""
    
    print("=== –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ë–ï–ù–ß–ú–ê–†–ö –°–ò–°–¢–ï–ú–´ ===\n")
    
    # 1. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
    print("1. GPU –°–¢–ê–¢–£–°:")
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"   GPU {i}: {props.name}")
            print(f"   –ü–∞–º—è—Ç—å: {props.total_memory / 1024**3:.1f} GB")
            print(f"   Compute: {props.major}.{props.minor}")
    else:
        print("   CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    # 2. –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
    provider = get_optimal_provider()
    print(f"\n2. –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ô –ü–†–û–í–ê–ô–î–ï–†: {provider.upper()}")
    
    # 3. –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    print("\n3. –¢–ï–°–¢ –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ï–ô:")
    models_to_test = ["reswapper128", "reswapper256"]
    
    for model_name in models_to_test:
        start_time = time.time()
        try:
            model = load_model(model_name, use_tensorrt=True, provider_type=provider)
            load_time = time.time() - start_time
            print(f"   {model_name}: {load_time:.2f}s ‚úÖ")
        except Exception as e:
            print(f"   {model_name}: –û–®–ò–ë–ö–ê - {e} ‚ùå")
    
    # 4. –ë–µ–Ω—á–º–∞—Ä–∫ inference
    print("\n4. –ë–ï–ù–ß–ú–ê–†–ö INFERENCE:")
    try:
        model = load_model("reswapper128", use_tensorrt=True, provider_type=provider)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if provider == "cuda":
            device = torch.device("cuda")
        else:
            device = torch.device("cpu")
            
        target = torch.randn(1, 3, 128, 128).to(device)
        source = torch.randn(1, 512).to(device)
        
        # –ü—Ä–æ–≥—Ä–µ–≤
        for _ in range(3):
            with torch.no_grad():
                _ = model(target, source)
        
        if provider == "cuda":
            torch.cuda.synchronize()
        
        # –ë–µ–Ω—á–º–∞—Ä–∫
        times = []
        for _ in range(10):
            start_time = time.time()
            with torch.no_grad():
                result = model(target, source)
            if provider == "cuda":
                torch.cuda.synchronize()
            times.append(time.time() - start_time)
        
        avg_time = np.mean(times)
        fps = 1.0 / avg_time
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.4f}s")
        print(f"   FPS: {fps:.1f}")
        
        # –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if fps >= 30:
            print("   –û—Ü–µ–Ω–∫–∞: –û–¢–õ–ò–ß–ù–û üöÄ")
        elif fps >= 15:
            print("   –û—Ü–µ–Ω–∫–∞: –•–û–†–û–®–û ‚úÖ")
        elif fps >= 5:
            print("   –û—Ü–µ–Ω–∫–∞: –ü–†–ò–ï–ú–õ–ï–ú–û ‚ö†Ô∏è")
        else:
            print("   –û—Ü–µ–Ω–∫–∞: –ú–ï–î–õ–ï–ù–ù–û ‚ùå")
            
    except Exception as e:
        print(f"   –û—à–∏–±–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}")
    
    print("\n=== –ë–ï–ù–ß–ú–ê–†–ö –ó–ê–í–ï–†–®–ï–ù ===")

# –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞
comprehensive_benchmark()
```

---

## üéØ TensorRT –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### –í–∫–ª—é—á–µ–Ω–∏–µ TensorRT

TensorRT –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç **3x —É—Å–∫–æ—Ä–µ–Ω–∏–µ** –¥–ª—è PyTorch –º–æ–¥–µ–ª–µ–π –Ω–∞ NVIDIA GPU.

```python
from liveswapping.ai_models.models import load_model

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–∫–ª—é—á–µ–Ω–∏–µ TensorRT (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
model = load_model("reswapper128", use_tensorrt=True)

# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
model = load_model("reswapper128", use_tensorrt=False)
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã TensorRT

```python
def check_tensorrt_status():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ TensorRT –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
    
    try:
        import torch_tensorrt
        print(f"‚úÖ torch-tensorrt —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {torch_tensorrt.__version__}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ CUDA
        cuda_version = torch.version.cuda
        print(f"‚úÖ PyTorch CUDA: {cuda_version}")
        
        # –¢–µ—Å—Ç –∫–æ–º–ø–∏–ª—è—Ü–∏–∏
        model = load_model("reswapper128", use_tensorrt=True)
        print("‚úÖ TensorRT –∫–æ–º–ø–∏–ª—è—Ü–∏—è —É—Å–ø–µ—à–Ω–∞")
        
        return True
        
    except ImportError:
        print("‚ùå torch-tensorrt –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch-tensorrt")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ TensorRT: {e}")
        return False

check_tensorrt_status()
```

### –†—É—á–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ TensorRT

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ torch-tensorrt
pip install torch-tensorrt

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
python -c "import torch_tensorrt; print(torch_tensorrt.__version__)"

# –î–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π CUDA
pip install torch-tensorrt --index-url https://download.pytorch.org/whl/cu118
```

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ TensorRT –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

```python
def create_custom_tensorrt_model(model, precision="fp32"):
    """–ö–∞—Å—Ç–æ–º–Ω–∞—è TensorRT –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è."""
    
    try:
        import torch_tensorrt
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        enabled_precisions = {torch.float32}
        if precision == "fp16":
            enabled_precisions.add(torch.half)
        
        # –ö–æ–º–ø–∏–ª—è—Ü–∏—è —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        compiled_model = torch_tensorrt.compile(
            model,
            inputs=[
                torch_tensorrt.Input((1, 3, 128, 128)),
                torch_tensorrt.Input((1, 512))
            ],
            enabled_precisions=enabled_precisions,
            ir="torch_compile",
            min_block_size=3,
            require_full_compilation=False,
        )
        
        return compiled_model
        
    except Exception as e:
        print(f"TensorRT compilation failed: {e}")
        return model
```

---

## üéÆ GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ

### CuPy –¥–ª—è numpy –æ–ø–µ—Ä–∞—Ü–∏–π

CuPy –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ numpy –æ–ø–µ—Ä–∞—Ü–∏–π.

```python
from liveswapping.utils.gpu_utils import GPUArrayManager

# –°–æ–∑–¥–∞–Ω–∏–µ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ GPU –º–∞—Å—Å–∏–≤–æ–≤
gpu_manager = GPUArrayManager(use_cupy=True, verbose=True)

def optimized_image_processing(image):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."""
    
    # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU
    gpu_image = gpu_manager.to_gpu(image)
    
    # GPU –æ–ø–µ—Ä–∞—Ü–∏–∏ (–ø—Ä–∏–º–µ—Ä)
    # gpu_result = cupy_operation(gpu_image)
    
    # –í–æ–∑–≤—Ä–∞—Ç –Ω–∞ CPU
    result = gpu_manager.to_cpu(gpu_image)
    gpu_manager.synchronize()
    
    return result
```

### –ë–µ–Ω—á–º–∞—Ä–∫ CuPy vs NumPy

```python
from liveswapping.utils.gpu_utils import analyze_cupy_performance

# –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ CuPy
analyze_cupy_performance()

# –í—ã–≤–æ–¥:
# Testing with 1000x1000 arrays:
# [CPU] CPU time: 0.0123s
# [GPU] GPU time: 0.0041s  
# [SPEEDUP] Speedup: 3.0x
```

### –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

```python
from liveswapping.utils.adaptive_cupy import create_adaptive_processor

# –°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
processor = create_adaptive_processor(1080)  # –î–ª—è 1080p

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
from liveswapping.utils.adaptive_cupy import AdaptiveColorTransfer, AdaptiveBlending

color_transfer = AdaptiveColorTransfer(processor)
blending = AdaptiveBlending(processor)

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä GPU/CPU –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞
result = color_transfer.apply_color_transfer_adaptive(source_path, target, face_analysis)
```

---

## üîÑ –í—ã–±–æ—Ä –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ

```python
from liveswapping.ai_models.models import get_optimal_provider, load_model

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª—É—á—à–µ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
provider = get_optimal_provider()
print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider}")

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º
model = load_model("reswapper128", provider_type=provider)
```

### –†—É—á–Ω–æ–π –≤—ã–±–æ—Ä –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

#### NVIDIA GPU (CUDA + TensorRT)
```python
# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è NVIDIA
model = load_model("reswapper128", 
                  provider_type="cuda", 
                  use_tensorrt=True)
```

#### AMD GPU (DirectML)
```python
# –î–ª—è AMD GPU
model = load_model("reswapper128", 
                  provider_type="directml")
```

#### Intel GPU/CPU (OpenVINO)
```python
# –î–ª—è Intel —É—Å—Ç—Ä–æ–π—Å—Ç–≤
model = load_model("reswapper128", 
                  provider_type="openvino")
```

#### CPU Only
```python
# –¢–æ–ª—å–∫–æ CPU (fallback)
model = load_model("reswapper128", 
                  provider_type="cpu",
                  use_tensorrt=False)
```

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤

```python
def benchmark_providers():
    """–ë–µ–Ω—á–º–∞—Ä–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤."""
    
    providers = ["cuda", "directml", "openvino", "cpu"]
    results = {}
    
    for provider in providers:
        try:
            print(f"\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {provider.upper()}...")
            
            start_time = time.time()
            model = load_model("reswapper128", 
                             provider_type=provider,
                             use_tensorrt=(provider == "cuda"))
            load_time = time.time() - start_time
            
            # –¢–µ—Å—Ç inference (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
            inference_times = []
            for _ in range(5):
                start = time.time()
                # –°–∏–º—É–ª—è—Ü–∏—è inference
                time.sleep(0.01)  # –ó–∞–≥–ª—É—à–∫–∞
                inference_times.append(time.time() - start)
            
            avg_inference = np.mean(inference_times)
            fps = 1.0 / avg_inference
            
            results[provider] = {
                'load_time': load_time,
                'inference_time': avg_inference,
                'fps': fps,
                'status': 'success'
            }
            
            print(f"   –ó–∞–≥—Ä—É–∑–∫–∞: {load_time:.2f}s")
            print(f"   Inference: {avg_inference:.4f}s")
            print(f"   FPS: {fps:.1f}")
            
        except Exception as e:
            results[provider] = {
                'status': 'failed',
                'error': str(e)
            }
            print(f"   –û—à–∏–±–∫–∞: {e}")
    
    # –õ—É—á—à–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
    successful = {k: v for k, v in results.items() if v['status'] == 'success'}
    if successful:
        best = max(successful.items(), key=lambda x: x[1]['fps'])
        print(f"\nüèÜ –õ—É—á—à–∏–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {best[0].upper()} ({best[1]['fps']:.1f} FPS)")
    
    return results

benchmark_providers()
```

---

## üß† –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π

### –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏

| –°—Ü–µ–Ω–∞—Ä–∏–π | –ú–æ–¥–µ–ª—å | –ü—Ä–æ–≤–∞–π–¥–µ—Ä | TensorRT | FPS (RTX 4090) |
|----------|--------|-----------|----------|----------------|
| **Real-time –º–∞–∫—Å–∏–º—É–º** | reswapper128 | CUDA | ‚úÖ | ~45 |
| **Real-time –∫–∞—á–µ—Å—Ç–≤–æ** | reswapper256 | CUDA | ‚úÖ | ~25 |
| **–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è** | inswapper128 | CUDA | ‚ùå | ~30 |
| **CPU —Ä–µ–∂–∏–º** | reswapper128 | CPU | ‚ùå | ~2 |

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ GPU –ø–∞–º—è—Ç–∏

```python
def optimize_for_memory(gpu_memory_gb):
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ–¥ –¥–æ—Å—Ç—É–ø–Ω—É—é GPU –ø–∞–º—è—Ç—å."""
    
    if gpu_memory_gb >= 12:
        # –í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–µ GPU (RTX 4090, 3090)
        return {
            'model': 'reswapper256',
            'batch_size': 4,
            'resolution': 256,
            'use_tensorrt': True,
            'upscaling': True
        }
    elif gpu_memory_gb >= 8:
        # –°—Ä–µ–¥–Ω–∏–µ GPU (RTX 4070, 3080)
        return {
            'model': 'reswapper256',
            'batch_size': 2,
            'resolution': 256,
            'use_tensorrt': True,
            'upscaling': True
        }
    elif gpu_memory_gb >= 4:
        # –ú–ª–∞–¥—à–∏–µ GPU (GTX 1660, RTX 3060)
        return {
            'model': 'reswapper128',
            'batch_size': 2,
            'resolution': 128,
            'use_tensorrt': True,
            'upscaling': False
        }
    else:
        # –û—á–µ–Ω—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å
        return {
            'model': 'reswapper128',
            'batch_size': 1,
            'resolution': 128,
            'use_tensorrt': False,
            'upscaling': False
        }

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
config = get_optimal_config()
settings = optimize_for_memory(config['memory_gb'])
print(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏: {settings}")
```

### –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π

```python
class ModelCache:
    """–ö—ç—à –¥–ª—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
    
    def __init__(self):
        self.models = {}
    
    def preload_models(self, model_names):
        """–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –≤ –ø–∞–º—è—Ç—å."""
        
        for name in model_names:
            print(f"–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ {name}...")
            self.models[name] = load_model(name, use_tensorrt=True)
            print(f"‚úÖ {name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    def get_model(self, name):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏."""
        return self.models.get(name)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
cache = ModelCache()
cache.preload_models(['reswapper128', 'reswapper256'])

# –ú–≥–Ω–æ–≤–µ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = cache.get_model('reswapper128')
```

---

## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã

### –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è CUDA
export CUDA_VISIBLE_DEVICES=0
export CUDA_LAUNCH_BLOCKING=0
export CUDA_CACHE_DISABLE=0

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è PyTorch
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
export TORCH_CUDNN_V8_API_ENABLED=1

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ verbose –ª–æ–≥–æ–≤
export ONNX_LOG_LEVEL=3
export OMP_NUM_THREADS=1

# –ü–∞–º—è—Ç—å
export PYTORCH_NO_CUDA_MEMORY_CACHING=0
```

### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Windows

```cmd
REM GPU –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
reg add "HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Multimedia\SystemProfile\Tasks\Games" /v "GPU Priority" /t REG_DWORD /d 8 /f

REM –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–∞
reg add "HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Multimedia\SystemProfile\Tasks\Games" /v "Priority" /t REG_DWORD /d 6 /f
```

### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Linux

```bash
# GPU performance mode
sudo nvidia-smi -pm 1
sudo nvidia-smi -ac 1215,2100  # –î–ª—è RTX 4090

# CPU governor
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# Swappiness
echo 10 | sudo tee /proc/sys/vm/swappiness
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Python

```python
# performance_config.py
import gc
import torch

def optimize_python_runtime():
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Python runtime –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
    
    # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ —Å–±–æ—Ä—â–∏–∫–∞ –º—É—Å–æ—Ä–∞ –≤–æ –≤—Ä–µ–º—è inference
    gc.disable()
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è PyTorch
    torch.backends.cudnn.benchmark = True  # –ê–≤—Ç–æ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è cuDNN
    torch.backends.cudnn.deterministic = False  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
    
    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        # –ü—Ä–æ–≥—Ä–µ–≤ GPU
        dummy = torch.randn(1000, 1000).cuda()
        dummy = dummy @ dummy
        del dummy
        torch.cuda.empty_cache()
    
    print("‚úÖ Python runtime –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω")

optimize_python_runtime()
```

---

## üìä –ë–µ–Ω—á–º–∞—Ä–∫–∏ –∏ —Ç–µ—Å—Ç—ã

### –ü–æ–ª–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
def full_performance_benchmark():
    """–ü–æ–ª–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã."""
    
    print("=== –ü–û–õ–ù–´–ô –ë–ï–ù–ß–ú–ê–†–ö –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò ===\n")
    
    components = [
        ("Model Loading", benchmark_model_loading),
        ("Inference Speed", benchmark_inference),
        ("Memory Usage", benchmark_memory),
        ("Upscaling", benchmark_upscaling),
        ("End-to-End", benchmark_end_to_end)
    ]
    
    results = {}
    
    for name, benchmark_func in components:
        print(f"üîÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {name}")
        try:
            result = benchmark_func()
            results[name] = result
            print(f"‚úÖ {name}: –∑–∞–≤–µ—Ä—à–µ–Ω–æ\n")
        except Exception as e:
            print(f"‚ùå {name}: –æ—à–∏–±–∫–∞ - {e}\n")
            results[name] = {'error': str(e)}
    
    # –û–±—â–∏–π –æ—Ç—á–µ—Ç
    print("=== –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ ===")
    for name, result in results.items():
        if 'error' not in result:
            print(f"‚úÖ {name}: {result.get('summary', 'OK')}")
        else:
            print(f"‚ùå {name}: {result['error']}")
    
    return results

def benchmark_model_loading():
    """–ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π."""
    
    models = ['reswapper128', 'reswapper256', 'inswapper128']
    times = {}
    
    for model_name in models:
        start_time = time.time()
        model = load_model(model_name, use_tensorrt=True)
        load_time = time.time() - start_time
        times[model_name] = load_time
        print(f"   {model_name}: {load_time:.2f}s")
    
    avg_time = np.mean(list(times.values()))
    return {
        'times': times,
        'average': avg_time,
        'summary': f'{avg_time:.2f}s avg'
    }

def benchmark_inference():
    """–ë–µ–Ω—á–º–∞—Ä–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ inference."""
    
    model = load_model('reswapper128', use_tensorrt=True)
    provider = get_optimal_provider()
    
    if provider == "cuda":
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    target = torch.randn(1, 3, 128, 128).to(device)
    source = torch.randn(1, 512).to(device)
    
    # –ü—Ä–æ–≥—Ä–µ–≤
    for _ in range(10):
        with torch.no_grad():
            _ = model(target, source)
    
    if provider == "cuda":
        torch.cuda.synchronize()
    
    # –ë–µ–Ω—á–º–∞—Ä–∫
    times = []
    for _ in range(50):
        start_time = time.time()
        with torch.no_grad():
            result = model(target, source)
        if provider == "cuda":
            torch.cuda.synchronize()
        times.append(time.time() - start_time)
    
    avg_time = np.mean(times)
    fps = 1.0 / avg_time
    
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.4f}s")
    print(f"   FPS: {fps:.1f}")
    
    return {
        'avg_time': avg_time,
        'fps': fps,
        'summary': f'{fps:.1f} FPS'
    }

def benchmark_memory():
    """–ë–µ–Ω—á–º–∞—Ä–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏."""
    
    if not torch.cuda.is_available():
        return {'summary': 'CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}
    
    # –ù–∞—á–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å
    torch.cuda.empty_cache()
    initial_memory = torch.cuda.memory_allocated(0)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model = load_model('reswapper128', use_tensorrt=True)
    model_memory = torch.cuda.memory_allocated(0) - initial_memory
    
    # Inference –ø–∞–º—è—Ç—å
    target = torch.randn(1, 3, 128, 128).cuda()
    source = torch.randn(1, 512).cuda()
    
    with torch.no_grad():
        result = model(target, source)
    
    peak_memory = torch.cuda.max_memory_allocated(0)
    
    memory_mb = {
        'model': model_memory / 1024**2,
        'peak': peak_memory / 1024**2,
        'current': torch.cuda.memory_allocated(0) / 1024**2
    }
    
    print(f"   –ú–æ–¥–µ–ª—å: {memory_mb['model']:.1f} MB")
    print(f"   –ü–∏–∫: {memory_mb['peak']:.1f} MB")
    
    return {
        'memory_mb': memory_mb,
        'summary': f"{memory_mb['peak']:.1f} MB peak"
    }

# –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞
results = full_performance_benchmark()
```

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫

```python
def compare_settings():
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
    
    settings = [
        {'name': '–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å', 'model': 'reswapper128', 'tensorrt': True, 'resolution': 128},
        {'name': '–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è', 'model': 'reswapper128', 'tensorrt': True, 'resolution': 256},
        {'name': '–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ', 'model': 'reswapper256', 'tensorrt': True, 'resolution': 256},
        {'name': 'CPU —Ä–µ–∂–∏–º', 'model': 'reswapper128', 'tensorrt': False, 'resolution': 128},
    ]
    
    print("=== –°–†–ê–í–ù–ï–ù–ò–ï –ù–ê–°–¢–†–û–ï–ö ===\n")
    
    for setting in settings:
        print(f"üîÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {setting['name']}")
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            start_time = time.time()
            model = load_model(
                setting['model'], 
                use_tensorrt=setting['tensorrt'],
                provider_type="cuda" if setting['tensorrt'] else "cpu"
            )
            load_time = time.time() - start_time
            
            # –¢–µ—Å—Ç inference
            device = torch.device("cuda" if setting['tensorrt'] else "cpu")
            res = setting['resolution']
            target = torch.randn(1, 3, res, res).to(device)
            source = torch.randn(1, 512).to(device)
            
            # –ü—Ä–æ–≥—Ä–µ–≤
            for _ in range(3):
                with torch.no_grad():
                    _ = model(target, source)
            
            # –ë–µ–Ω—á–º–∞—Ä–∫
            times = []
            for _ in range(10):
                start = time.time()
                with torch.no_grad():
                    result = model(target, source)
                if setting['tensorrt']:
                    torch.cuda.synchronize()
                times.append(time.time() - start)
            
            avg_time = np.mean(times)
            fps = 1.0 / avg_time
            
            print(f"   –ó–∞–≥—Ä—É–∑–∫–∞: {load_time:.2f}s")
            print(f"   FPS: {fps:.1f}")
            print(f"   –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ: {res}x{res}")
            print("")
            
        except Exception as e:
            print(f"   –û—à–∏–±–∫–∞: {e}\n")

compare_settings()
```

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

```python
def auto_optimize():
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ–¥ —Å–∏—Å—Ç–µ–º—É."""
    
    print("=== –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø ===\n")
    
    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã
    config = get_optimal_config()
    provider = get_optimal_provider()
    
    print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider.upper()}")
    print(f"–ü–∞–º—è—Ç—å GPU: {config['memory_gb']:.1f} GB")
    
    # –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
    if provider == "cuda" and config['memory_gb'] >= 8:
        optimal_settings = {
            'model': 'reswapper256',
            'use_tensorrt': True,
            'resolution': 256,
            'batch_size': 2,
            'upscaling': True
        }
        expected_fps = "20-30"
    elif provider == "cuda" and config['memory_gb'] >= 4:
        optimal_settings = {
            'model': 'reswapper128',
            'use_tensorrt': True,
            'resolution': 128,
            'batch_size': 2,
            'upscaling': True
        }
        expected_fps = "30-45"
    elif provider in ["directml", "openvino"]:
        optimal_settings = {
            'model': 'reswapper128',
            'use_tensorrt': False,
            'resolution': 128,
            'batch_size': 1,
            'upscaling': False
        }
        expected_fps = "10-20"
    else:
        optimal_settings = {
            'model': 'reswapper128',
            'use_tensorrt': False,
            'resolution': 128,
            'batch_size': 1,
            'upscaling': False
        }
        expected_fps = "2-5"
    
    print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –ù–ê–°–¢–†–û–ô–ö–ò:")
    for key, value in optimal_settings.items():
        print(f"   {key}: {value}")
    print(f"   –û–∂–∏–¥–∞–µ–º—ã–π FPS: {expected_fps}")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
    print(f"\nüîÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫...")
    
    try:
        model = load_model(
            optimal_settings['model'],
            use_tensorrt=optimal_settings['use_tensorrt'],
            provider_type=provider
        )
        
        print("‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")
        print("üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫: {e}")
    
    return optimal_settings

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
optimal = auto_optimize()
```

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é

### Real-time —Ä–µ–∂–∏–º
```bash
# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
python -m liveswapping.run realtime \
    --source face.jpg \
    --modelPath models/reswapper128.pth \
    --resolution 128 \
    --delay 0

# –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
python -m liveswapping.run realtime \
    --source face.jpg \
    --modelPath models/reswapper256.pth \
    --resolution 256 \
    --mouth_mask
```

### –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ
```bash
# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
python -m liveswapping.run video \
    --source face.jpg \
    --target_video video.mp4 \
    --modelPath models/reswapper256.pth \
    --upscale 2 \
    --bg_upsampler realesrgan \
    --weight 0.8

# –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
python -m liveswapping.run video \
    --source face.jpg \
    --target_video video.mp4 \
    --modelPath models/reswapper128.pth \
    --resolution 128
```

---

## üîó –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- **[üè† Home](Home)** - –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ wiki
- **[üéØ Quick Start](Quick-Start)** - –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç  
- **[üîß Troubleshooting](Troubleshooting)** - –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º
- **[üìã API Reference](API-Reference)** - –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ API

---

*[‚¨ÖÔ∏è API Reference](API-Reference) | [üè† –ì–ª–∞–≤–Ω–∞—è](Home)*